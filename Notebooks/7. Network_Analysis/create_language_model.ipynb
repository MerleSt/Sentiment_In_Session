{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-15T10:30:34.866201Z",
     "start_time": "2023-11-15T10:30:34.853151Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext, StorageContext, load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "import gradio as gr\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:23:40.927221Z",
     "start_time": "2023-11-14T18:23:40.916557Z"
    }
   },
   "id": "26f59d5f1bc66cc4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:21:41.380902Z",
     "start_time": "2023-11-14T18:21:40.410708Z"
    }
   },
   "id": "d32f1391fa97be95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ede5ae78dd328af6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "api_key = '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'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:22:18.392163Z",
     "start_time": "2023-11-14T18:22:18.381559Z"
    }
   },
   "id": "86f995379e56b535"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gradio' has no attribute 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 27\u001B[0m\n\u001B[1;32m     23\u001B[0m     response \u001B[38;5;241m=\u001B[39m index\u001B[38;5;241m.\u001B[39mquery(input_text, response_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompact\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39mresponse\n\u001B[1;32m     26\u001B[0m iface \u001B[38;5;241m=\u001B[39m gr\u001B[38;5;241m.\u001B[39mInterface(fn\u001B[38;5;241m=\u001B[39mchatbot,\n\u001B[0;32m---> 27\u001B[0m                      inputs\u001B[38;5;241m=\u001B[39mgr\u001B[38;5;241m.\u001B[39minputs\u001B[38;5;241m.\u001B[39mTextbox(lines\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m7\u001B[39m, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEnter your text\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     28\u001B[0m                      outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     29\u001B[0m                      title\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMy AI Chatbot\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     31\u001B[0m index \u001B[38;5;241m=\u001B[39m construct_index(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     32\u001B[0m iface\u001B[38;5;241m.\u001B[39mlaunch(share\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'gradio' has no attribute 'inputs'"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "def construct_index(directory_path):\n",
    "    max_input_size = 4096\n",
    "    num_outputs = 512\n",
    "    max_chunk_overlap = 20\n",
    "    chunk_size_limit = 600\n",
    "\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.7, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n",
    "\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "\n",
    "    index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "    index.save_to_disk('index.json')\n",
    "\n",
    "    return index\n",
    "\n",
    "def chatbot(input_text):\n",
    "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
    "    response = index.query(input_text, response_mode=\"compact\")\n",
    "    return response.response\n",
    "\n",
    "iface = gr.Interface(fn=chatbot,\n",
    "                     inputs=gr.inputs.Textbox(lines=7, label=\"Enter your text\"),\n",
    "                     outputs=\"text\",\n",
    "                     title=\"My AI Chatbot\")\n",
    "\n",
    "index = construct_index(\"docs\")\n",
    "iface.launch(share=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:23:42.694383Z",
     "start_time": "2023-11-14T18:23:42.682301Z"
    }
   },
   "id": "6e12bd12ff19e694"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_csv('/Users/merlesteffen/Documents/Education/WBS_Coding_School/Bootcamp/Sentiment_In_Session/Final_DF/sentiment_analysis_all.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T10:30:46.654603Z",
     "start_time": "2023-11-15T10:30:37.710202Z"
    }
   },
   "id": "6d477b0eb2ff688a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df = df_sentiment.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T10:30:46.977372Z",
     "start_time": "2023-11-15T10:30:46.655313Z"
    }
   },
   "id": "93981dd14f9de5c7"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0  Sitzung        Date            Start          Schluss  \\\n0     3600000      143  2020-01-30  0 days 09:00:00  0 days 19:01:00   \n1     3600001      143  2020-01-30  0 days 09:00:00  0 days 19:01:00   \n2     3600002      143  2020-01-30  0 days 09:00:00  0 days 19:01:00   \n3     3600003      143  2020-01-30  0 days 09:00:00  0 days 19:01:00   \n4     3600004      143  2020-01-30  0 days 09:00:00  0 days 19:01:00   \n\n                          Speaker  \\\n0  Vizepräsident Wolfgang Kubicki   \n1  Vizepräsident Wolfgang Kubicki   \n2  Vizepräsident Wolfgang Kubicki   \n3  Vizepräsident Wolfgang Kubicki   \n4  Vizepräsident Wolfgang Kubicki   \n\n                                         Text_Spoken Reactions  \\\n0                    Der Antrag ist damit abgelehnt.       NaN   \n1  Endgültiges Ergebnis Abgegebene Stimmen: 633;d...       NaN   \n2  Sybille Benning Dr André Berghegger Melanie Be...       NaN   \n3  Dr Reinhard Brandl Sebastian Brehm Heike Brehm...       NaN   \n4                          Fischer (Karlsruhe Land).       NaN   \n\n               Name Fraktion_x       Position  Wahlperiode  positive  \\\n0  Wolfgang Kubicki        FDP  Abgeordnete*r         20.0  0.069543   \n1  Wolfgang Kubicki        FDP  Abgeordnete*r         20.0  0.493157   \n2  Wolfgang Kubicki        FDP  Abgeordnete*r         20.0  0.582360   \n3  Wolfgang Kubicki        FDP  Abgeordnete*r         20.0  0.531848   \n4  Wolfgang Kubicki        FDP  Abgeordnete*r         20.0  0.557220   \n\n   negative   neutral  \n0  0.737363  0.193094  \n1  0.350404  0.156440  \n2  0.243855  0.173785  \n3  0.315269  0.152883  \n4  0.229822  0.212958  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Sitzung</th>\n      <th>Date</th>\n      <th>Start</th>\n      <th>Schluss</th>\n      <th>Speaker</th>\n      <th>Text_Spoken</th>\n      <th>Reactions</th>\n      <th>Name</th>\n      <th>Fraktion_x</th>\n      <th>Position</th>\n      <th>Wahlperiode</th>\n      <th>positive</th>\n      <th>negative</th>\n      <th>neutral</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3600000</td>\n      <td>143</td>\n      <td>2020-01-30</td>\n      <td>0 days 09:00:00</td>\n      <td>0 days 19:01:00</td>\n      <td>Vizepräsident Wolfgang Kubicki</td>\n      <td>Der Antrag ist damit abgelehnt.</td>\n      <td>NaN</td>\n      <td>Wolfgang Kubicki</td>\n      <td>FDP</td>\n      <td>Abgeordnete*r</td>\n      <td>20.0</td>\n      <td>0.069543</td>\n      <td>0.737363</td>\n      <td>0.193094</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3600001</td>\n      <td>143</td>\n      <td>2020-01-30</td>\n      <td>0 days 09:00:00</td>\n      <td>0 days 19:01:00</td>\n      <td>Vizepräsident Wolfgang Kubicki</td>\n      <td>Endgültiges Ergebnis Abgegebene Stimmen: 633;d...</td>\n      <td>NaN</td>\n      <td>Wolfgang Kubicki</td>\n      <td>FDP</td>\n      <td>Abgeordnete*r</td>\n      <td>20.0</td>\n      <td>0.493157</td>\n      <td>0.350404</td>\n      <td>0.156440</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3600002</td>\n      <td>143</td>\n      <td>2020-01-30</td>\n      <td>0 days 09:00:00</td>\n      <td>0 days 19:01:00</td>\n      <td>Vizepräsident Wolfgang Kubicki</td>\n      <td>Sybille Benning Dr André Berghegger Melanie Be...</td>\n      <td>NaN</td>\n      <td>Wolfgang Kubicki</td>\n      <td>FDP</td>\n      <td>Abgeordnete*r</td>\n      <td>20.0</td>\n      <td>0.582360</td>\n      <td>0.243855</td>\n      <td>0.173785</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3600003</td>\n      <td>143</td>\n      <td>2020-01-30</td>\n      <td>0 days 09:00:00</td>\n      <td>0 days 19:01:00</td>\n      <td>Vizepräsident Wolfgang Kubicki</td>\n      <td>Dr Reinhard Brandl Sebastian Brehm Heike Brehm...</td>\n      <td>NaN</td>\n      <td>Wolfgang Kubicki</td>\n      <td>FDP</td>\n      <td>Abgeordnete*r</td>\n      <td>20.0</td>\n      <td>0.531848</td>\n      <td>0.315269</td>\n      <td>0.152883</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3600004</td>\n      <td>143</td>\n      <td>2020-01-30</td>\n      <td>0 days 09:00:00</td>\n      <td>0 days 19:01:00</td>\n      <td>Vizepräsident Wolfgang Kubicki</td>\n      <td>Fischer (Karlsruhe Land).</td>\n      <td>NaN</td>\n      <td>Wolfgang Kubicki</td>\n      <td>FDP</td>\n      <td>Abgeordnete*r</td>\n      <td>20.0</td>\n      <td>0.557220</td>\n      <td>0.229822</td>\n      <td>0.212958</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T10:30:46.988996Z",
     "start_time": "2023-11-15T10:30:46.979480Z"
    }
   },
   "id": "619b1080203470b9"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# Filter speakers with at least 2000 sentences\n",
    "speaker_counts = df['Name'].value_counts()\n",
    "selected_speakers = speaker_counts[speaker_counts >= 20000].index\n",
    "filtered_df = df[df['Name'].isin(selected_speakers)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:11:20.780270Z",
     "start_time": "2023-11-14T18:11:20.470086Z"
    }
   },
   "id": "dfd1f97f62545fe"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Petra Pau', 'Claudia Roth', 'Hermann Otto Solms', 'Norbert Lammert',\n       'Wolfgang Kubicki', 'Wolfgang Thierse', 'Susanne Kastner',\n       'Peter Friedrich', 'Wolfgang Schäuble', 'Ulla Schmidt',\n       'Edelgard Bulmahn', 'Gerda Hasselfeldt', 'Thomas Oppermann',\n       'Eduard Oswald'],\n      dtype='object')"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_speakers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:11:26.128563Z",
     "start_time": "2023-11-14T18:11:26.116069Z"
    }
   },
   "id": "2f8160b2d81e05dc"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Combine text for each speaker\n",
    "speaker_texts = filtered_df.groupby('Name')['Text_Spoken'].apply(' '.join)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:11:51.889780Z",
     "start_time": "2023-11-14T18:11:51.764394Z"
    }
   },
   "id": "c68e029af4a7b2a2"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "gpt3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:261\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 261\u001B[0m     response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/requests/models.py:1021\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1021\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/gpt3/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRepositoryNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:429\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    427\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    428\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 429\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m hf_hub_download(\n\u001B[1;32m    430\u001B[0m         path_or_repo_id,\n\u001B[1;32m    431\u001B[0m         filename,\n\u001B[1;32m    432\u001B[0m         subfolder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(subfolder) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m subfolder,\n\u001B[1;32m    433\u001B[0m         repo_type\u001B[38;5;241m=\u001B[39mrepo_type,\n\u001B[1;32m    434\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m    435\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m    436\u001B[0m         user_agent\u001B[38;5;241m=\u001B[39muser_agent,\n\u001B[1;32m    437\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m    438\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m    439\u001B[0m         resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[1;32m    440\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m    441\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    442\u001B[0m     )\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    116\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 118\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1346\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001B[0m\n\u001B[1;32m   1344\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, RepositoryNotFoundError) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, GatedRepoError):\n\u001B[1;32m   1345\u001B[0m     \u001B[38;5;66;03m# Repo not found => let's raise the actual error\u001B[39;00m\n\u001B[0;32m-> 1346\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m head_call_error\n\u001B[1;32m   1347\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1348\u001B[0m     \u001B[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1232\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001B[0m\n\u001B[1;32m   1231\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1232\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m get_hf_file_metadata(\n\u001B[1;32m   1233\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m   1234\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m   1235\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m   1236\u001B[0m         timeout\u001B[38;5;241m=\u001B[39metag_timeout,\n\u001B[1;32m   1237\u001B[0m     )\n\u001B[1;32m   1238\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n\u001B[1;32m   1239\u001B[0m     \u001B[38;5;66;03m# Cache the non-existence of the file and raise\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    116\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 118\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1608\u001B[0m, in \u001B[0;36mget_hf_file_metadata\u001B[0;34m(url, token, proxies, timeout)\u001B[0m\n\u001B[1;32m   1599\u001B[0m r \u001B[38;5;241m=\u001B[39m _request_wrapper(\n\u001B[1;32m   1600\u001B[0m     method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHEAD\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1601\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1606\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m   1607\u001B[0m )\n\u001B[0;32m-> 1608\u001B[0m hf_raise_for_status(r)\n\u001B[1;32m   1610\u001B[0m \u001B[38;5;66;03m# Return\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:293\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    285\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    286\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Client Error.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    287\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    291\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m make sure you are authenticated.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    292\u001B[0m     )\n\u001B[0;32m--> 293\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RepositoryNotFoundError(message, response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    295\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m400\u001B[39m:\n",
      "\u001B[0;31mRepositoryNotFoundError\u001B[0m: 401 Client Error. (Request ID: Root=1-6553b882-20cfcef455b4de167a801ea8;0dd969b9-86ef-4990-8bae-c09d255617f2)\n\nRepository Not Found for url: https://huggingface.co/gpt3/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Example: Fine-tuning process (high-level)\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m GPT2Tokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgpt3\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m GPT2LMHeadModel\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgpt3\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1940\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1937\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizer_file\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m vocab_files:\n\u001B[1;32m   1938\u001B[0m     \u001B[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001B[39;00m\n\u001B[1;32m   1939\u001B[0m     fast_tokenizer_file \u001B[38;5;241m=\u001B[39m FULL_TOKENIZER_FILE\n\u001B[0;32m-> 1940\u001B[0m     resolved_config_file \u001B[38;5;241m=\u001B[39m cached_file(\n\u001B[1;32m   1941\u001B[0m         pretrained_model_name_or_path,\n\u001B[1;32m   1942\u001B[0m         TOKENIZER_CONFIG_FILE,\n\u001B[1;32m   1943\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m   1944\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m   1945\u001B[0m         resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[1;32m   1946\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m   1947\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m   1948\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m   1949\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m   1950\u001B[0m         subfolder\u001B[38;5;241m=\u001B[39msubfolder,\n\u001B[1;32m   1951\u001B[0m         user_agent\u001B[38;5;241m=\u001B[39muser_agent,\n\u001B[1;32m   1952\u001B[0m         _raise_exceptions_for_missing_entries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1953\u001B[0m         _raise_exceptions_for_connection_errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1954\u001B[0m         _commit_hash\u001B[38;5;241m=\u001B[39mcommit_hash,\n\u001B[1;32m   1955\u001B[0m     )\n\u001B[1;32m   1956\u001B[0m     commit_hash \u001B[38;5;241m=\u001B[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001B[1;32m   1957\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m resolved_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:450\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    444\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    445\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to access a gated repo.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mMake sure to request access at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    446\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and pass a token having permission to this repo either \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    447\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    448\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    449\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 450\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    451\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a local folder and is not a valid model identifier \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    452\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlisted on \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mIf this is a private repository, make sure to pass a token \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    453\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    454\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`token=<your_token>`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    455\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RevisionNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    458\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrevision\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    459\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor this model name. Check the model page at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    460\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for available revisions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    461\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mOSError\u001B[0m: gpt3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "# Example: Fine-tuning process (high-level)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt3')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt3')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:18.971963Z",
     "start_time": "2023-11-14T18:12:18.541189Z"
    }
   },
   "id": "750925f5d1381834"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let us train GPT on our speakers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1922bed957614e3f"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Name\nAgnieszka Brugger           Herr Präsident! Meine Damen und Herren! In kei...\nAlbert Rupprecht(Weiden)    Sehr geehrte Frau Präsidentin! Liebe Kolleginn...\nAlbrecht Glaser             Herr Präsident! Meine sehr verehrten Damen und...\nAlexander Bonde             Herr Präsident! Verehrte Damen und Herren! Lie...\nAlexander Dobrindt          Herr Präsident! Sehr geehrte Damen und Herren!...\n                                                  ...                        \nWolfgang Wiehle             Sehr geehrter Herr Präsident! Kolleginnen und ...\nWolfgang Wieland            Frau Staatsekretärin, grüne Parlamentarier sin...\nWolfgang Zöller             Grüß Gott, Herr Präsident! Liebe Kolleginnen u...\nYvonne Magwas               Sehr geehrter Herr Präsident! Liebe Kolleginne...\nÖzcan Mutlu                 Sehr geehrter Herr Präsident! Liebe Kolleginne...\nName: Text_Spoken, Length: 722, dtype: object"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_texts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:41:50.332007Z",
     "start_time": "2023-11-14T15:41:50.330151Z"
    }
   },
   "id": "7b64352abe44b7b4"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:42:42.005928Z",
     "start_time": "2023-11-14T15:42:41.994536Z"
    }
   },
   "id": "e9988533361bb7e2"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class SpeakerDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:58:01.220599Z",
     "start_time": "2023-11-14T15:58:01.218443Z"
    }
   },
   "id": "2e17987dabfcadcb"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "dataset = SpeakerDataset(speaker_texts, tokenizer, max_length=512)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:58:01.676161Z",
     "start_time": "2023-11-14T15:58:01.671525Z"
    }
   },
   "id": "91b9d8d24e0d0fa"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:59:07.744478Z",
     "start_time": "2023-11-14T15:59:07.736803Z"
    }
   },
   "id": "d2b58ee6b251d17a"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:59:44.632889Z",
     "start_time": "2023-11-14T15:59:44.622414Z"
    }
   },
   "id": "5957414560abb29a"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Function for training one epoch\n",
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model = model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = input_ids.clone()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return total_loss / len(data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:59:46.799169Z",
     "start_time": "2023-11-14T15:59:46.797920Z"
    }
   },
   "id": "55377f75ca74eb77"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Function for evaluating the model\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = input_ids.clone()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:59:47.045130Z",
     "start_time": "2023-11-14T15:59:47.043397Z"
    }
   },
   "id": "d4adf3c972bc3ad4"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training and validation\n",
    "epochs = 4  # Choose the number of epochs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T16:00:28.950480Z",
     "start_time": "2023-11-14T16:00:28.940390Z"
    }
   },
   "id": "9c88f9602a4e2c6a"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Assuming you have a 'dataset' variable which is an instance of your custom Dataset class\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)  # No need to shuffle the validation set"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T16:38:13.970565Z",
     "start_time": "2023-11-14T16:38:13.960284Z"
    }
   },
   "id": "502ab3b6a06c027c"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3.6319022469404265, Val Loss: 3.366922950744629\n",
      "Epoch 2, Train Loss: 3.5257807882820686, Val Loss: 3.3341230869293215\n",
      "Epoch 3, Train Loss: 3.496351201359819, Val Loss: 3.3341230869293215\n",
      "Epoch 4, Train Loss: 3.502487589673298, Val Loss: 3.3341230869293215\n"
     ]
    },
    {
     "data": {
      "text/plain": "('/Users/merlesteffen/Documents/Education/WBS_Coding_School/Bootcamp/Sentiment_In_Session/training_models_on_politicians/tokenizer_config.json',\n '/Users/merlesteffen/Documents/Education/WBS_Coding_School/Bootcamp/Sentiment_In_Session/training_models_on_politicians/special_tokens_map.json',\n '/Users/merlesteffen/Documents/Education/WBS_Coding_School/Bootcamp/Sentiment_In_Session/training_models_on_politicians/vocab.json',\n '/Users/merlesteffen/Documents/Education/WBS_Coding_School/Bootcamp/Sentiment_In_Session/training_models_on_politicians/merges.txt',\n '/Users/merlesteffen/Documents/Education/WBS_Coding_School/Bootcamp/Sentiment_In_Session/training_models_on_politicians/added_tokens.json')"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set pad_token to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = evaluate_model(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('/Users/merlesteffen/Documents/Education/WBS_Coding_School/Bootcamp/Sentiment_In_Session/training_models_on_politicians')\n",
    "tokenizer.save_pretrained('/Users/merlesteffen/Documents/Education/WBS_Coding_School/Bootcamp/Sentiment_In_Session/training_models_on_politicians')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T17:38:01.261482Z",
     "start_time": "2023-11-14T16:38:15.882593Z"
    }
   },
   "id": "8ec9d09303b4c15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "275930dcc397c92f"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schreibe einen Text wie Petra Pau. Herr Präsident! Liebe Kolleginnen und Kollegen! Ich muss einmal einmal einmal einmal einmal einmal\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('/Users/merlesteffen/Documents/Education/WBS_Coding_School/Bootcamp/Sentiment_In_Session/training_models_on_politicians')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('/Users/merlesteffen/Documents/Education/WBS_Coding_School/Bootcamp/Sentiment_In_Session/training_models_on_politicians')\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(prompt, length=50):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    attention_mask = torch.ones(input_ids.shape, device=input_ids.device)\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=length, num_return_sequences=1)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Schreibe einen Text wie Petra Pau.\"  # Replace with your own prompt\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:10:23.122747Z",
     "start_time": "2023-11-14T18:10:20.912289Z"
    }
   },
   "id": "a4c2d2ace1969000"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "961c1a0954da8eec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
